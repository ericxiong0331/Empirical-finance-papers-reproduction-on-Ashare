{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Out-of-Sample Equity Premium Prediction: Combination Forecasts and Links to the Real Economy\n",
    "\n",
    "Source: https://doi.org/10.1093/rfs/hhp063\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "The study preprocesses market and macroeconomic data, constructs a set of predictive variables, and generates out-of-sample equity premium forecasts using rolling regression models. \n",
    "\n",
    "It implements both single-factor and multi-factor combination forecasting methods, including simple (mean, median, trimmed mean) and weighted (DMSPE) approaches.\n",
    "\n",
    "The predictive performance of each model is evaluated using metrics such as mean squared error (MSE), out-of-sample R², statistical significance (p-values), and economic returns. \n",
    "\n",
    "Visualization routines are included to illustrate time-series prediction error and comparative model effectiveness.\n",
    "\n",
    "The results show that ensemble forecast combinations consistently outperform single-factor predictors in terms of accuracy and stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.filterwarnings('ignore', category=pd.errors.SettingWithCopyWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing and Feature Engineering\n",
    "\n",
    "Domestic data is missing for B/M (Book-to-Market Ratio), company bond yield-related variables such as DFY and DFR, and macroeconomic variables such as INFL and I/K. Perform combined forecasting using the remaining ten variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of price-related variables for the CSI 300 index: D/P, D/Y, P/E, SVAR\n",
    "pedata = pd.read_excel(\"000300.SH-历史PE_PB-20241202.xlsx\", header=0)\n",
    "pedata[\"lastclsprc\"] = pedata[\"收盘价\"].shift(1)\n",
    "pedata[\"交易日期\"] = pd.to_datetime(pedata[\"交易日期\"],format=\"%Y-%m-%d\")\n",
    "pedata[\"Year\"] = pedata[\"交易日期\"].dt.year\n",
    "pedata[\"Month\"] = pedata[\"交易日期\"].dt.month\n",
    "pedata.loc[pedata[\"Month\"]<=3,\"Season\"] =1\n",
    "pedata.loc[(pedata[\"Month\"]>=3) & (pedata[\"Month\"]<=6),\"Season\"] =2\n",
    "pedata.loc[(pedata[\"Month\"]>=6) & (pedata[\"Month\"]<=9),\"Season\"] =3\n",
    "pedata.loc[(pedata[\"Month\"]>=9) & (pedata[\"Month\"]<=12),\"Season\"] =4\n",
    "pedata[\"Dividend\"] = pedata[\"收盘价\"] * pedata[\"股息率\"]/100\n",
    "pedata[\"D/P\"] = np.log(pedata[\"股息率\"])\n",
    "\n",
    "data = pedata.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"D/P\"].mean()).reset_index()\n",
    "pedata[\"D/Y\"] = np.log(pedata[\"Dividend\"]/pedata[\"lastclsprc\"])\n",
    "data1 = pedata.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"D/Y\"].mean()).reset_index()\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")\n",
    "pedata[\"E/P\"] = -np.log(pedata[\"市盈率TTM\"])\n",
    "data1 = pedata.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"E/P\"].mean()).reset_index()\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")\n",
    "pedata[\"SVAR\"] = ((pedata[\"收盘价\"] - pedata[\"lastclsprc\"])/pedata[\"lastclsprc\"])**2\n",
    "data1 = pedata.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"SVAR\"].sum()).reset_index()\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"outer\",suffixes=('', '_y'))\n",
    "data.columns = [\"Year\",\"Season\",\"D/P\",\"D/Y\",\"P/E\",\"SVAR\"]\n",
    "data.drop([len(data)-1],inplace=True)\n",
    "data.dropna(inplace=True)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the CSI 300 index dividend payout ratio variable (D/E)\n",
    "statementdata = pd.read_excel(\"000300.SH-财务数据升-20241202.xlsx\").T\n",
    "statementdata.reset_index(inplace=True)\n",
    "statementdata.iloc[0,1] =\"Season\"\n",
    "statementdata.columns = statementdata.iloc[0,:]\n",
    "statementdata.drop([0],inplace=True)\n",
    "\n",
    "k=1\n",
    "for i in statementdata[\"Season\"].unique():\n",
    "    statementdata.loc[statementdata[\"Season\"]==i,\"Season\"] = k\n",
    "    k+=1\n",
    "statementdata[\"Year\"] = pd.to_datetime(statementdata[\"报告期\"], format=\"%Y-%m-%d\").dt.year\n",
    "\n",
    "data1 = pedata.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"Dividend\"].mean()).reset_index()\n",
    "data1.drop([0,1],inplace=True)\n",
    "data1 = data1.merge(statementdata[[\"Year\",\"Season\",\"归属母公司股东的净利润\"]], on=[\"Year\",\"Season\"], how=\"outer\")\n",
    "data1.dropna(inplace=True)\n",
    "data1.columns = [\"Year\",\"Season\",\"Dividend\",\"NetProfit\"]\n",
    "data1[\"D/E\"] = np.log(data1[\"Dividend\"]/data1[\"NetProfit\"].apply(lambda x: x.replace(\",\",\"\")).astype(float))\n",
    "data = data.merge(data1[[\"Year\",\"Season\",\"D/E\"]], on=[\"Year\",\"Season\"], how=\"outer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the net equity expansion variable (NTIS) for Shanghai A-shares\n",
    "Annualcap = pd.read_csv(\"TRD_Yearm.csv\")\n",
    "Annualcap = Annualcap[(Annualcap[\"Markettype\"]==1) | (Annualcap[\"Markettype\"]==32)].groupby(\"Trdynt\").apply(lambda x: x[\"Ymvttl\"].sum()).reset_index()\n",
    "Annualcap.index = range(len(Annualcap))\n",
    "Annualcap.columns = [\"Year\",\"AnnualCap\"]\n",
    "Annualcap[\"Season\"] = 4\n",
    "\n",
    "issueSH = pd.read_csv(\"IPO_Ipobasic.csv\")\n",
    "issueSH = issueSH[(issueSH[\"Stkcd\"]<700000) & (issueSH[\"Stkcd\"]>=600000) & (issueSH[\"T1\"]==\"A\")]\n",
    "issueSH = issueSH.sort_values(by=\"Listdt\").groupby(\"Listdt\").sum().reset_index()\n",
    "issueSH.drop(issueSH[issueSH[\"Nshripo\"]==0].index, axis =0,inplace=True)\n",
    "issueSH[\"Year\"] = pd.to_datetime(issueSH[\"Listdt\"], format=\"%Y-%m-%d\").dt.year\n",
    "issueSH[\"Month\"] = pd.to_datetime(issueSH[\"Listdt\"], format=\"%Y-%m-%d\").dt.month\n",
    "issueSH.loc[issueSH[\"Month\"]<=3,\"Season\"] =1\n",
    "issueSH.loc[(issueSH[\"Month\"]>=3) & (issueSH[\"Month\"]<=6),\"Season\"] =2\n",
    "issueSH.loc[(issueSH[\"Month\"]>=6) & (issueSH[\"Month\"]<=9),\"Season\"] =3\n",
    "issueSH.loc[(issueSH[\"Month\"]>=9) & (issueSH[\"Month\"]<=12),\"Season\"] =4\n",
    "issueSH = issueSH.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"Nshripo\"].sum()).reset_index()\n",
    "issueSH.columns = [\"Year\",\"Season\",\"Nshripo\"]\n",
    "issue14 = issueSH.iloc[0:5,:]\n",
    "issueSH = data[[\"Year\",\"Season\"]].merge(issueSH, on=[\"Year\",\"Season\"], how=\"outer\")\n",
    "issueSH.fillna(0,inplace=True)\n",
    "issueSH.sort_values(by=[\"Year\",\"Season\"],inplace=True)\n",
    "\n",
    "issueSH[\"issueTTM\"] = issueSH[\"Nshripo\"].rolling(4).sum()\n",
    "issueSH = issueSH.merge(Annualcap, on=[\"Year\",\"Season\"], how=\"outer\")\n",
    "issueSH.ffill(inplace=True)\n",
    "issueSH.drop(issueSH.iloc[0:5,:].index, axis=0,inplace=True)\n",
    "issueSH[\"NTIS\"] = issueSH[\"issueTTM\"]/issueSH[\"AnnualCap\"]*100\n",
    "\n",
    "data1 = issueSH[[\"Year\",\"Season\",\"NTIS\"]]\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the 3-month treasury bill rate variable (using yield as a replacement)\n",
    "ytm = pd.read_csv(\"BND_TreasYield.csv\")\n",
    "ytm025 = ytm[(ytm[\"Cvtype\"]==1)&(ytm[\"Yeartomatu\"]==0.25)]\n",
    "ytm025[\"Year\"] = pd.to_datetime(ytm025[\"Trddt\"], format=\"%Y-%m-%d\").dt.year\n",
    "ytm025[\"Month\"] = pd.to_datetime(ytm025[\"Trddt\"], format=\"%Y-%m-%d\").dt.month\n",
    "ytm025.loc[ytm025[\"Month\"]<=3, \"Season\"] = 1\n",
    "ytm025.loc[(ytm025[\"Month\"]>=3) & (ytm025[\"Month\"]<=6), \"Season\"] = 2\n",
    "ytm025.loc[(ytm025[\"Month\"]>=6) & (ytm025[\"Month\"]<=9), \"Season\"] = 3\n",
    "ytm025.loc[(ytm025[\"Month\"]>=9) & (ytm025[\"Month\"]<=12), \"Season\"] = 4\n",
    "\n",
    "data1 = ytm025.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"Yield\"].mean()).reset_index()\n",
    "data1.rename({0:\"TBL\"},axis=1,inplace=True)\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the long-term government bond yield variable (LTY)\n",
    "ytm = ytm[(ytm[\"Cvtype\"]==1)&(ytm[\"Yeartomatu\"]==10)]\n",
    "ytm[\"Year\"] = pd.to_datetime(ytm[\"Trddt\"], format=\"%Y-%m-%d\").dt.year\n",
    "ytm[\"Month\"] = pd.to_datetime(ytm[\"Trddt\"], format=\"%Y-%m-%d\").dt.month\n",
    "ytm.loc[ytm[\"Month\"]<=3, \"Season\"] = 1\n",
    "ytm.loc[(ytm[\"Month\"]>=3) & (ytm[\"Month\"]<=6), \"Season\"] = 2\n",
    "ytm.loc[(ytm[\"Month\"]>=6) & (ytm[\"Month\"]<=9), \"Season\"] = 3\n",
    "ytm.loc[(ytm[\"Month\"]>=9) & (ytm[\"Month\"]<=12), \"Season\"] = 4\n",
    "\n",
    "data1 = ytm.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"Yield\"].mean()).reset_index()\n",
    "data1.rename({0:\"LTY\"},axis=1,inplace=True)\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the long-term government bond return variable (LTR)\n",
    "Binfo = pd.read_csv(\"BND_Florate.csv\")\n",
    "interrate = Binfo[(Binfo[\"Term\"]==10)]\n",
    "\n",
    "interrate[\"Year\"] = pd.to_datetime(interrate[\"Stopdt\"], format=\"%Y-%m-%d\").dt.year\n",
    "interrate[\"Month\"] = pd.to_datetime(interrate[\"Stopdt\"], format=\"%Y-%m-%d\").dt.month\n",
    "interrate.loc[interrate[\"Month\"]<=3, \"Season\"] = 1\n",
    "interrate.loc[(interrate[\"Month\"]>=3) & (interrate[\"Month\"]<=6), \"Season\"] = 2\n",
    "interrate.loc[(interrate[\"Month\"]>=6) & (interrate[\"Month\"]<=9), \"Season\"] = 3\n",
    "interrate.loc[(interrate[\"Month\"]>=9) & (interrate[\"Month\"]<=12), \"Season\"] = 4\n",
    "\n",
    "data1 = interrate.groupby([\"Year\",\"Season\"]).apply(lambda x: x[\"Intrrate\"].mean()).reset_index()\n",
    "data1.rename({0:\"LTR\"},axis=1,inplace=True)\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"outer\")\n",
    "data.drop(data.iloc[-1:,:].index,axis=0,inplace=True)\n",
    "data.ffill(inplace=True) #Forward fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction of the term spread variable (TMS, difference between long-term and short-term Treasury yields)\n",
    "data[\"TMS\"] = data[\"LTY\"] - data[\"TBL\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk Premium\n",
    "rpremium = pd.read_csv(\"STK_MKT_THRFACMONTH.csv\")\n",
    "rpremium = rpremium[rpremium[\"MarkettypeID\"]==\"P9701\"]\n",
    "rpremium[[\"Year\",\"Month\"]] = rpremium[\"TradingMonth\"].str.split(\"-\",expand=True)\n",
    "rpremium[\"Month\"] = rpremium[\"Month\"].astype(int)\n",
    "rpremium[\"Year\"] = rpremium[\"Year\"].astype(int)\n",
    "rpremium.loc[rpremium[\"Month\"]<=3, \"Season\"] = 1\n",
    "rpremium.loc[(rpremium[\"Month\"]>3) & (rpremium[\"Month\"]<=6), \"Season\"] = 2\n",
    "rpremium.loc[(rpremium[\"Month\"]>6) & (rpremium[\"Month\"]<=9), \"Season\"] = 3\n",
    "rpremium.loc[(rpremium[\"Month\"]>9) & (rpremium[\"Month\"]<=12), \"Season\"] = 4\n",
    "data1 = rpremium.groupby([\"Year\", \"Season\"]).apply(lambda x: x[\"RiskPremium2\"].sum()).reset_index()\n",
    "data1.rename({0:\"RiskPremium\"},axis=1,inplace=True)\n",
    "data = data.merge(data1, on=[\"Year\",\"Season\"], how=\"inner\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Univariate Forecast Model Training and Regression\n",
    "\n",
    "Perform regression and forecasting on one-period lagged excess returns; 'predict' contains forecasts from each individual variable. **Compared with historical mean of the return as baseline model**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = 25 #1/3 of data\n",
    "values = pd.DataFrame()\n",
    "predict = data.loc[train:,[\"Year\",\"Season\",\"RiskPremium\"]].reset_index(drop=True)\n",
    "for k in range(len(data)-train):\n",
    "    predict.loc[k,\"r_Histmean\"] = data.loc[1:train+k-1,\"RiskPremium\"].mean()\n",
    "data.columns[2:-1]\n",
    "for i in data.columns[2:-1]:\n",
    "    values[i] = sm.OLS(data.loc[:train,\"RiskPremium\"],sm.add_constant(data.loc[:train,i])).fit().params.values\n",
    "    predict[i] = values.loc[0,i]+values.loc[1,i]*data.loc[train-1:len(data)-1,i].reset_index(drop=True)\n",
    "predict.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combined Model and  Forecast Construction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combined Model contains two types of combining method. \n",
    "\n",
    "First type includes mean, median and trimmed mean of ten singular variate.  \n",
    "\n",
    "Second type includes DMSPE prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasts from the first type of combined forecasting\n",
    "values.index = [\"alpha\",\"beta\"]\n",
    "combpredict = predict[[\"Year\",\"Season\"]]\n",
    "combpredict[\"r_Histmean\"] = predict[\"r_Histmean\"]\n",
    "combpredict[\"RiskPremium\"] = data.loc[train:,\"RiskPremium\"].reset_index(drop=True)\n",
    "combpredict[\"Mean\"] = predict.iloc[:,2:].mean(axis=1)\n",
    "combpredict[\"Median\"] = predict.iloc[:,2:].median(axis=1)\n",
    "combpredict[\"TrimmedMean\"] = (predict.iloc[:,2:].sum(axis=1)-predict.iloc[:,2:].max(axis=1)-predict.iloc[:,2:].min(axis=1))/(predict.iloc[:,2:].shape[1]-2)\n",
    "combpredict_1 = combpredict.copy()\n",
    "combpredict_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecasts from the second type of combined forecasting\n",
    "# Theta is 1 or 0.9，q0 is holdout period of out-of-sample period\n",
    "def DMSPE_prediction(theta,q0):\n",
    "    fai = pd.DataFrame()\n",
    "    for j in range(len(predict)-q0):\n",
    "        for i in predict.columns[4:]:\n",
    "            fai.loc[j,i] = (np.full(q0, theta) ** range(q0-1,-1,-1)) @ ((predict.loc[j:j+q0-1,\"RiskPremium\"]-predict.loc[j:j+q0-1,i])**2).values.T\n",
    "    weights = (1/fai).div((1/fai).sum(axis=1),axis=0)\n",
    "    new_predict = (weights.values * predict.iloc[q0:,4:].values).sum(axis=1)\n",
    "    return new_predict\n",
    "\n",
    "q0 = 15 # manually setting holdout period\n",
    "combpredict = combpredict.iloc[q0:,:].reset_index(drop=True)\n",
    "combpredict[\"DMSPE_0.9\"] = DMSPE_prediction(0.9,q0)\n",
    "combpredict[\"DMSPE_1\"] = DMSPE_prediction(1,q0)\n",
    "combpredict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Plotting and Visualization\n",
    "\n",
    "Generate the mean squared error (MSE) between historical average predictions and actual returns, and the difference with the MSE between individual variable predictions and actual returns. Cumulative error plot is shown in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "j=1\n",
    "for i in predict.columns[4:]:\n",
    "    serie = ((predict[\"RiskPremium\"]-predict[\"r_Histmean\"])**2)-((predict[\"RiskPremium\"]-predict[i])**2)\n",
    "    cumsum_serie = np.cumsum(serie)\n",
    "    plt.subplot(3,4,j)\n",
    "    plt.subplots_adjust(hspace=0.5,wspace=0.3)\n",
    "    plt.plot(cumsum_serie)\n",
    "    plt.title(i)\n",
    "    plt.axhline(0, color='black', lw=1)\n",
    "    plt.ylim(-0.1,0.1)\n",
    "    j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the MSE between historical average predictions and actual returns, and the difference with the MSE between combined forecasts and actual returns. Cumulative error plot is shown in the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "j=1\n",
    "for i in combpredict.columns[4:]:\n",
    "    serie = ((combpredict[\"RiskPremium\"]-combpredict[\"r_Histmean\"])**2)-((combpredict[\"RiskPremium\"]-combpredict[i])**2)\n",
    "    cumsum_serie = np.cumsum(serie)\n",
    "    plt.subplot(2,3,j)\n",
    "    plt.subplots_adjust(hspace=0.5,wspace=0.3)\n",
    "    plt.plot(cumsum_serie)\n",
    "    plt.title(i)\n",
    "    plt.axhline(0, color='black', lw=1)\n",
    "    plt.ylim(-0.1,0.1)\n",
    "    j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: It is shown that only P/E, D/E, LTR has a relative low cumulative error among univariate models. \n",
    "\n",
    "Comparing to univariate models, combined models predict cumulative error on a greater stability, while the variation of the error of second type of combination is much more robust."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. R2OS Metric and Significance Testing\n",
    "\n",
    "R²OS (Out-of-sample R-squared) measures a model’s predictive performance out of sample; higher positive values indicate better out-of-sample explanatory power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R^2OS values for individual variable and combined forecasts\n",
    "r2os = pd.DataFrame()\n",
    "for i in predict.columns[4:]:\n",
    "    r2os.loc[i,\"R2OS\"] = 1 - np.sum((predict[i]-predict[\"RiskPremium\"])**2)/np.sum((predict[\"RiskPremium\"]-predict[\"r_Histmean\"])**2)\n",
    "for i in combpredict.columns[4:]:\n",
    "    r2os.loc[i,\"R2OS\"] = 1 - np.sum((combpredict[i]-combpredict[\"RiskPremium\"])**2)/np.sum((combpredict[\"RiskPremium\"]-combpredict[\"r_Histmean\"])**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p-values from the adjusted MSPE test of R^2OS for individual variable and combined forecasts.\n",
    "for i in predict.columns[4:]:\n",
    "    ft = (predict[\"RiskPremium\"]-predict[\"r_Histmean\"])**2 - (predict[i]-predict[\"RiskPremium\"])**2 + (predict[\"RiskPremium\"]-predict[\"r_Histmean\"])**2\n",
    "    r2os.loc[i,\"p_value\"] = sm.OLS(ft,np.ones(len(ft))).fit().pvalues.iloc[0]\n",
    "for i in combpredict.columns[4:]:\n",
    "    ft = (combpredict[\"RiskPremium\"]-combpredict[\"r_Histmean\"])**2 - (combpredict[i]-combpredict[\"RiskPremium\"])**2 + (combpredict[\"RiskPremium\"]-combpredict[\"r_Histmean\"])**2\n",
    "    r2os.loc[i,\"p_value\"] = sm.OLS(ft,np.ones(len(ft))).fit().pvalues.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast evaluation: Mean-variance investor utility\n",
    "estimate = data[[\"Year\",\"Season\"]].copy()\n",
    "estimate[\"r_var\"] = data[\"RiskPremium\"].rolling(4).var().shift(1)\n",
    "estimate = estimate.merge(predict, on=[\"Year\",\"Season\"], how=\"inner\")\n",
    "estimate = estimate.merge(combpredict, on=[\"Year\",\"Season\",\"RiskPremium\",\"r_Histmean\"], how=\"inner\")\n",
    "gamma = 3 # same as the paper setting\n",
    "weight_bench = (1/gamma)*(estimate['r_Histmean']/estimate['r_var'])\n",
    "return_bench = estimate['RiskPremium']*weight_bench\n",
    "cer_bench = return_bench.mean() - 0.5*gamma*(return_bench.var())\n",
    "\n",
    "for i in estimate.columns[5:]:\n",
    "    weight_model = (1/gamma)*(estimate[i]/estimate['r_var'])\n",
    "    return_model = estimate['RiskPremium']*weight_model\n",
    "    cer_model = return_model.mean() - 0.5*gamma*(return_model.var())\n",
    "    r2os.loc[i,\"economic_return\"] = cer_model - cer_bench\n",
    "r2os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings:\n",
    "Most single-factor predictors (e.g., D/P, D/Y, SVAR) report negative R²OS, meaning they fail to outperform the benchmark in out-of-sample prediction.\n",
    "\n",
    "P/E and LTR show small positive R²OS, but the improvement is limited.\n",
    "\n",
    "Combined models (Mean, Median, TrimmedMean, DMSPE) all have positive R²OS, showing significantly stronger out-of-sample predictive ability. Mean reaches 0.228929, the best among the displayed results.\n",
    "\n",
    "Combination model p-values shows great significance, and their economic returns are positive, further confirming effectiveness.\n",
    "\n",
    "Single factors rarely deliver robust out-of-sample results. Ensemble or aggregated methods provide substantial improvements in predictive power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Difference in mean squared error between individual variable forecasts and combined forecasts are shown in the series graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,6))\n",
    "j=1\n",
    "for i in predict.columns[4:]:\n",
    "    serie = ((predict[\"RiskPremium\"]-predict[\"r_Histmean\"])**2)-((predict[\"RiskPremium\"]-predict[i])**2)\n",
    "    plt.subplot(3,5,j)\n",
    "    plt.subplots_adjust(hspace=0.5,wspace=0.3)\n",
    "    plt.plot(serie)\n",
    "    plt.title(i)\n",
    "    plt.axhline(0, color='black', lw=1)\n",
    "    plt.ylim(-0.05,0.05)\n",
    "    j+=1\n",
    "for i in combpredict.columns[4:]:\n",
    "    serie = ((combpredict[\"RiskPremium\"]-combpredict[\"r_Histmean\"])**2)-((combpredict[\"RiskPremium\"]-combpredict[i])**2)\n",
    "    plt.subplot(3,5,j)\n",
    "    plt.subplots_adjust(hspace=0.5,wspace=0.3)\n",
    "    plt.plot(serie)\n",
    "    plt.title(i)\n",
    "    plt.axhline(0, color='black', lw=1)\n",
    "    plt.ylim(-0.05,0.05)\n",
    "    j+=1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings: Most single-factor models (e.g., D/P, D/Y, SVAR, TBL) exhibit large and volatile MSE, with occasional spikes, suggesting limited predictive precision and stability.\n",
    "\n",
    "Ensemble methods (Mean, Median, TrimmedMean, DMSPE_0.9, DMSPE_1) generally maintain lower and more stable MSE, especially the DMSPE series, which demonstrates the most robust performance.\n",
    "\n",
    "Single-factor forecasting is prone to higher and more unstable errors, while multi-factor ensemble models substantially reduce prediction error and improve consistency. Ensemble approaches are thus preferable for practical implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyself",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
